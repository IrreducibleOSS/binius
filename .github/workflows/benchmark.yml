name: Benchmark
run-name: >-
  ${{ github.event_name == 'workflow_dispatch' &&
      format('Benchmark: "{0}" (on-demand)', inputs.benchmark) ||
      format('Benchmark: "{0}" (commit)', github.event.head_commit.message)
  }}

on:
  push:
    branches: [ main ]
  workflow_dispatch:  # Manual trigger
    inputs:
      benchmark:
        description: "Benchmark to run (choose a specific benchmark or 'all')"
        type: choice
        options: [all, keccakf, groestl, vision32b, sha256, b32_mul, u32_add, u32_mul, xor, and, or]   # Example options; replace with actual benchmark names
        default: all
      publish_results:
        description: "Publish results to Bencher and GH Pages (Warning: run only if needed for main branch)"
        type: boolean
        default: false
jobs:
  #
  # Prepare
  #
  prepare:
    runs-on: ubuntu-latest
    outputs:
      publish_results: ${{ steps.set.outputs.publish_results }}
    steps:
      - id: set
        run: |
          echo "publish_results=${{ 
            (github.event_name == 'workflow_dispatch' && github.event.inputs.publish_results == 'true') ||
            (github.event_name == 'push' && github.ref_name == 'main')
          }}" >> $GITHUB_OUTPUT
  #
  # Run benchmarks
  #
  benchmark:
    name: Run benchmarks (${{ matrix.os }})
    needs: prepare
    container: rustlang/rust:nightly
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false  # make sure one matrix‐fail doesn’t stop the others
      matrix:
        os: [ c7a-2xlarge, c8g-2xlarge, supermicro ]
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetch all history to include all git information in traces
      - name: Set safe directory
        # workaround: https://github.com/actions/checkout/issues/2031
        run: git config --global --add safe.directory "$GITHUB_WORKSPACE"
      - name: Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
      - name: Execute Benchmarks
        run: |
          ./scripts/run_benchmark.py \
            --clean \
            --output-dir benchmark_results \
            --benchmark "${{ github.event.inputs.benchmark || 'all' }}"
      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.os }}
          path: benchmark_results
      - name: Prepare raw results for Bencher
        if: ${{ needs.prepare.outputs.publish_results == 'true' }}
        run: cp benchmark_results/all-results.json raw-results-${{ matrix.os }}.json
      - name: Upload raw results for Bencher
        if: ${{ needs.prepare.outputs.publish_results == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: upload-raw-results-${{ matrix.os }}
          path: raw-results-${{ matrix.os }}.json
  #
  # Publish results to Bencher
  #
  publish_to_bencher:
    name: Publish Results to Bencher (${{ matrix.os }})
    if: ${{ needs.prepare.outputs.publish_results == 'true' && needs.benchmark.result == 'success' }}
    permissions:
      contents: read # allow reading repository contents
      checks: write  # allow creating/updating check runs
    needs: [ benchmark, prepare ]
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        # Using matrix to workaround Bencher's limitation of publishing checks for multiple machines
        os: [ c7a-2xlarge, c8g-2xlarge, supermicro ]
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      - name: Setup Bencher
        uses: bencherdev/bencher@v0.4.37  # Pin to specific version to avoid breaking changes
      - name: Download raw results for Bencher
        uses: actions/download-artifact@v4
        with:
          pattern: upload-raw-results-${{ matrix.os }}
          path: raw_results  # directory for artifacts
          merge-multiple: true  # Merge multiple artifacts into a single directory
      - name: List results
        run: ls -lah raw_results/*
      - name: Convert raw to Bencher format
        run: |
          mkdir -p output
          ./scripts/convert_to_bencher.py \
            "raw_results/raw-results-${{ matrix.os }}.json" \
            "output/result.json"
      - name: Publish results to Bencher (${{ matrix.os }})
        env:
          BENCHER_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GHA_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GIT_BRANCH: ${{ github.ref_name }}
          MACHINE: ${{ matrix.os }}
        run: |
          bencher run \
            --project binius \
            --token "${BENCHER_TOKEN}" \
            --branch "${GIT_BRANCH}" \
            --testbed "${MACHINE}" \
            --threshold-measure latency \
            --threshold-test t_test \
            --threshold-max-sample-size 64 \
            --threshold-upper-boundary 0.99 \
            --thresholds-reset \
            --err \
            --adapter json \
            --github-actions "${GHA_TOKEN}" \
            --file "output/result.json"
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        if: ${{ matrix.os == 'c7a-2xlarge' }}
        with:
          name: gh-pages
          path: output/
  #
  # Publish results to GH Pages
  #
  publish_to_gh_pages:
    name: Publish Results to Github Page
    if: ${{ always() && github.ref_name == 'main' && needs.prepare.outputs.publish_results == 'true' }}
    permissions:
      contents: write
    needs: [ prepare, publish_to_bencher ]
    runs-on: ubuntu-latest
    steps:
      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: gh-pages
      - name: Deploy to GitHub Pages
        uses: crazy-max/ghaction-github-pages@v4
        with:
          repo: irreducibleoss/binius-benchmark
          fqdn: benchmark.binius.xyz
          target_branch: main
          build_dir: ./
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}
