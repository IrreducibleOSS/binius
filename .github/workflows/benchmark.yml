name: Benchmark
run-name: >-
  ${{ github.event_name == 'workflow_dispatch' &&
      format('Benchmark: "{0}" (on-demand)', inputs.benchmark) ||
      format('Benchmark: "{0}" (commit)', github.event.head_commit.message)
  }}

on:
  push:
    branches: [ main ]
  workflow_dispatch:  # Manual trigger
    inputs:
      benchmark:
        description: "Benchmark to run (choose a specific benchmark or 'all')"
        type: choice
        options: [all, keccakf, groestl, vision32b, sha256, b32_mul, u32_add, u32_mul, xor, and, or]   # Example options; replace with actual benchmark names
        default: all
      publish_results:
        description: "Publish results to Bencher and GH Pages (Warning: run only if needed for main branch)"
        type: boolean
        default: false
jobs:
  #
  # Prepare
  #
  prepare:
    runs-on: ubuntu-latest
    outputs:
      publish_results: ${{ steps.set.outputs.publish_results }}
    steps:
      - id: set
        run: |
          echo "publish_results=${{ 
            (github.event_name == 'workflow_dispatch' && github.event.inputs.publish_results == 'true') ||
            (github.event_name == 'push' && github.ref_name == 'main')
          }}" >> $GITHUB_OUTPUT
  #
  # Run benchmarks
  #
  benchmark:
    name: Run benchmarks (${{ matrix.os }})
    needs: prepare
    container: rustlang/rust:nightly
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false  # make sure one matrix‚Äêfail doesn‚Äôt stop the others
      matrix:
        os: [ c7a-2xlarge, c8g-2xlarge, supermicro ]
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetch all history to include all git information in traces
      - name: Set safe directory
        # workaround: https://github.com/actions/checkout/issues/2031
        run: git config --global --add safe.directory "$GITHUB_WORKSPACE"
      - name: Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
      - name: Execute Benchmarks
        run: |
          ./scripts/run_benchmark.py \
            --clean \
            --output-dir benchmark_results \
            --benchmark "${{ github.event.inputs.benchmark || 'all' }}"
      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.os }}
          path: benchmark_results
      - name: Prepare raw results for Bencher
        if: ${{ needs.prepare.outputs.publish_results == 'true' }}
        run: cp benchmark_results/all-results.json raw-results-${{ matrix.os }}.json
      - name: Upload raw results for Bencher
        if: ${{ needs.prepare.outputs.publish_results == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: upload-raw-results-${{ matrix.os }}
          path: raw-results-${{ matrix.os }}.json
  #
  # Publish results to Bencher
  #
  publish_to_bencher:
    name: Publish Results to Bencher (${{ matrix.os }})
    if: ${{ needs.prepare.outputs.publish_results == 'true' && needs.benchmark.result == 'success' }}
    permissions:
      contents: read # allow reading repository contents
      checks: write  # allow creating/updating check runs
    needs: [ benchmark, prepare ]
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        # Using matrix to workaround Bencher's limitation of publishing checks for multiple machines
        os: [ c7a-2xlarge, c8g-2xlarge, supermicro ]
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      - name: Setup Bencher
        uses: bencherdev/bencher@v0.4.37  # Pin to specific version to avoid breaking changes
      - name: Download raw results for Bencher
        uses: actions/download-artifact@v4
        with:
          pattern: upload-raw-results-${{ matrix.os }}
          path: raw_results  # directory for artifacts
          merge-multiple: true  # Merge multiple artifacts into a single directory
      - name: List results
        run: ls -lah raw_results/*
      - name: Convert raw to Bencher format
        run: |
          mkdir -p output
          ./scripts/convert_to_bencher.py \
            "raw_results/raw-results-${{ matrix.os }}.json" \
            "output/result.json"
      - name: Publish results to Bencher (${{ matrix.os }})
        env:
          BENCHER_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GHA_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GIT_BRANCH: ${{ github.ref_name }}
          MACHINE: ${{ matrix.os }}
        run: |
          bencher run \
            --project binius \
            --token "${BENCHER_TOKEN}" \
            --branch "${GIT_BRANCH}" \
            --testbed "${MACHINE}" \
            --threshold-measure latency \
            --threshold-test t_test \
            --threshold-max-sample-size 64 \
            --threshold-upper-boundary 0.99 \
            --thresholds-reset \
            --err \
            --adapter json \
            --github-actions "${GHA_TOKEN}" \
            --file "output/result.json"
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        if: ${{ matrix.os == 'c7a-2xlarge' }}
        with:
          name: gh-pages
          path: output/
  #
  # Publish results to GH Pages
  #
  publish_to_gh_pages:
    name: Publish Results to Github Page
    if: ${{ github.ref_name == 'main' && needs.prepare.outputs.publish_results == 'true' }}
    permissions:
      contents: write
    needs: [ prepare, publish_to_bencher ]
    runs-on: ubuntu-latest
    steps:
      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: gh-pages
      - name: Deploy to GitHub Pages
        uses: crazy-max/ghaction-github-pages@v4
        with:
          repo: irreducibleoss/binius-benchmark
          fqdn: benchmark.binius.xyz
          target_branch: main
          build_dir: ./
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}
  #
  # Upload perfetto traces to s3
  #
  upload_perfetto_traces:
    name: Upload Perfetto Traces to S3
    permissions:
      contents: read # allow reading repository contents
      checks: write  # allow creating/updating check runs
      id-token: write # Required to get AWS credentials with OIDC
    needs: [ benchmark, prepare ]
    runs-on: ubuntu-latest
    steps:
      - name: Download Prefetto Traces
        uses: actions/download-artifact@v4
        with:
          pattern: results-*
          path: benchmark_results  # directory for artifacts
      - name: List results
        run: ls -lah benchmark_results/*

      - name: Set up Python 3
        uses: actions/setup-python@v5
        with:
          python-version: 3.13
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
            aws-region: us-west-2
            role-to-assume: ${{ secrets.AWS_UPLOAD_ROLE }}
      - name: Upload Perfetto Traces
        run: |
          python3 - <<'EOF'
          import os, glob, subprocess, urllib.parse, datetime

          perfetto_host = "https://perfetto.irreducible.com"
          s3_bucket     = "${{ secrets.PERFETTO_BUCKET }}"
          repo          = "${{ github.repository }}".split("/",1)[1]
          branch        = "${{ github.ref_name }}".replace("/", "-")
          sha           = "${{ github.sha }}"[:7]
          run_dir       = f"{datetime.datetime.now(datetime.UTC):%Y-%m-%d_%H-%M-%S}-{sha}"
          summary_path  = os.environ["GITHUB_STEP_SUMMARY"]

          traces_by_benchmark = {}

          # find & upload
          for fp in sorted(glob.glob("benchmark_results/**/*.perfetto-trace", recursive=True)):
              fn = os.path.basename(fp)
              machine = os.path.basename(os.path.dirname(fp)).removeprefix("results-")
              parts = fn.split("-", 4)
              bm, mode, _, run_id, _ = parts
              thread = mode + "-thread"

              s3_key = f"traces/{repo}/{branch}/{bm}/{thread}/{machine}/{run_dir}/{machine}-{fn}"
              subprocess.run(["aws", "s3", "cp", fp, f"{s3_bucket}/{s3_key}"], check=True)

              trace_url       = f"{perfetto_host}/{s3_key}"
              perfetto_ui_url = f"{perfetto_host}/#!/?url={urllib.parse.quote_plus(trace_url)}"

              traces_by_benchmark.setdefault(bm, {}).setdefault(f"{bm} ({thread}) on {machine}", []).append(
                f"<a href='{perfetto_ui_url}' target='_blank'>#{run_id}</a>"
              )

          # Write Summary to GitHub
          with open(summary_path, "a") as summary:
            summary.write("## üìä Perfetto Traces\n")
            for bm, traces_by_group in traces_by_benchmark.items():
              summary.write(f"<details><summary>{bm}</summary>\n<ul>\n")
              for group_name, traces in traces_by_group.items():
                summary.write(f"<li>{group_name} ")
                summary.write(", ".join(traces))
                summary.write("</li>\n")
              summary.write("</ul>\n</details>\n")
          EOF
